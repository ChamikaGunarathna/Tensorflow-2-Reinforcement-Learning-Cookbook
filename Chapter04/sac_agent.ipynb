{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft Actor-Critic (SAC) agent training script\n",
    "Chapter 4, TensorFlow 2 Reinforcement Learning Cookbook | Praveen Palanisamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import functools\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crypto_trading_continuous_env import CryptoTradingContinuousEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(state_shape, action_shape, units=(512, 256, 64)):\n",
    "    state_shape_flattened = functools.reduce(lambda x, y: x * y, state_shape)\n",
    "    state = Input(shape=state_shape_flattened)\n",
    "    x = Dense(units[0], name=\"L0\", activation=\"relu\")(state)\n",
    "    for index in range(1, len(units)):\n",
    "        x = Dense(units[index], name=\"L{}\".format(index), activation=\"relu\")(x)\n",
    "\n",
    "    actions_mean = Dense(action_shape[0], name=\"Out_mean\")(x)\n",
    "    actions_std = Dense(action_shape[0], name=\"Out_std\")(x)\n",
    "\n",
    "    model = Model(inputs=state, outputs=[actions_mean, actions_std], name=\"Actor\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic(state_shape, action_shape, units=(512, 256, 64)):\n",
    "    state_shape_flattened = functools.reduce(lambda x, y: x * y, state_shape)\n",
    "    inputs = [Input(shape=state_shape_flattened), Input(shape=action_shape)]\n",
    "    concat = Concatenate(axis=-1)(inputs)\n",
    "    x = Dense(units[0], name=\"Hidden0\", activation=\"relu\")(concat)\n",
    "    for index in range(1, len(units)):\n",
    "        x = Dense(units[index], name=\"Hidden{}\".format(index), activation=\"relu\")(x)\n",
    "\n",
    "    output = Dense(1, name=\"Out_QVal\")(x)\n",
    "    model = Model(inputs=inputs, outputs=output, name=\"Critic\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_weights(model, target_model, tau=0.005):\n",
    "    weights = model.get_weights()\n",
    "    target_weights = target_model.get_weights()\n",
    "    for i in range(len(target_weights)):  # set tau% of target model to be new weights\n",
    "        target_weights[i] = weights[i] * tau + target_weights[i] * (1 - tau)\n",
    "    target_model.set_weights(target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        lr_actor=3e-5,\n",
    "        lr_critic=3e-4,\n",
    "        actor_units=(64, 64),\n",
    "        critic_units=(64, 64),\n",
    "        auto_alpha=True,\n",
    "        alpha=0.2,\n",
    "        tau=0.005,\n",
    "        gamma=0.99,\n",
    "        batch_size=128,\n",
    "        memory_cap=100000,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.state_shape = env.observation_space.shape  # shape of observations\n",
    "        self.action_shape = env.action_space.shape  # number of actions\n",
    "        self.action_bound = (env.action_space.high - env.action_space.low) / 2\n",
    "        self.action_shift = (env.action_space.high + env.action_space.low) / 2\n",
    "        self.memory = deque(maxlen=int(memory_cap))\n",
    "\n",
    "        # Define and initialize actor network\n",
    "        self.actor = actor(self.state_shape, self.action_shape, actor_units)\n",
    "        self.actor_optimizer = Adam(learning_rate=lr_actor)\n",
    "        self.log_std_min = -20\n",
    "        self.log_std_max = 2\n",
    "        print(self.actor.summary())\n",
    "\n",
    "        # Define and initialize critic networks\n",
    "        self.critic_1 = critic(self.state_shape, self.action_shape, critic_units)\n",
    "        self.critic_target_1 = critic(self.state_shape, self.action_shape, critic_units)\n",
    "        self.critic_optimizer_1 = Adam(learning_rate=lr_critic)\n",
    "        update_target_weights(self.critic_1, self.critic_target_1, tau=1.0)\n",
    "\n",
    "        self.critic_2 = critic(self.state_shape, self.action_shape, critic_units)\n",
    "        self.critic_target_2 = critic(self.state_shape, self.action_shape, critic_units)\n",
    "        self.critic_optimizer_2 = Adam(learning_rate=lr_critic)\n",
    "        update_target_weights(self.critic_2, self.critic_target_2, tau=1.0)\n",
    "\n",
    "        print(self.critic_1.summary())\n",
    "\n",
    "        # Define and initialize temperature alpha and target entropy\n",
    "        self.auto_alpha = auto_alpha\n",
    "        if auto_alpha:\n",
    "            self.target_entropy = -np.prod(self.action_shape)\n",
    "            self.log_alpha = tf.Variable(0.0, dtype=tf.float64)\n",
    "            self.alpha = tf.Variable(0.0, dtype=tf.float64)\n",
    "            self.alpha.assign(tf.exp(self.log_alpha))\n",
    "            self.alpha_optimizer = Adam(learning_rate=lr_actor)\n",
    "        else:\n",
    "            self.alpha = tf.Variable(alpha, dtype=tf.float64)\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.tau = tau  # target model update\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Tensorboard\n",
    "        self.summaries = {}\n",
    "\n",
    "    def process_actions(self, mean, log_std, test=False, eps=1e-6):\n",
    "        std = tf.math.exp(log_std)\n",
    "        raw_actions = mean\n",
    "\n",
    "        if not test:\n",
    "            raw_actions += tf.random.normal(shape=mean.shape, dtype=tf.float64) * std\n",
    "\n",
    "        log_prob_u = tfp.distributions.Normal(loc=mean, scale=std).log_prob(raw_actions)\n",
    "        actions = tf.math.tanh(raw_actions)\n",
    "\n",
    "        log_prob = tf.reduce_sum(log_prob_u - tf.math.log(1 - actions ** 2 + eps))\n",
    "\n",
    "        actions = actions * self.action_bound + self.action_shift\n",
    "\n",
    "        return actions, log_prob\n",
    "\n",
    "    def act(self, state, test=False, use_random=False):\n",
    "        state = state.reshape(-1)  # Flatten state\n",
    "        state = np.expand_dims(state, axis=0).astype(np.float64)\n",
    "\n",
    "        if use_random:\n",
    "            a = tf.random.uniform(\n",
    "                shape=(1, self.action_shape[0]), minval=-1, maxval=1, dtype=tf.float64\n",
    "            )\n",
    "        else:\n",
    "            means, log_stds = self.actor.predict(state)\n",
    "            log_stds = tf.clip_by_value(log_stds, self.log_std_min, self.log_std_max)\n",
    "\n",
    "            a, log_prob = self.process_actions(means, log_stds, test=test)\n",
    "\n",
    "        q1 = self.critic_1.predict([state, a])[0][0]\n",
    "        q2 = self.critic_2.predict([state, a])[0][0]\n",
    "        self.summaries[\"q_min\"] = tf.math.minimum(q1, q2)\n",
    "        self.summaries[\"q_mean\"] = np.mean([q1, q2])\n",
    "\n",
    "        return a\n",
    "\n",
    "    def save_model(self, a_fn, c_fn):\n",
    "        self.actor.save(a_fn)\n",
    "        self.critic_1.save(c_fn)\n",
    "\n",
    "    def load_actor(self, a_fn):\n",
    "        self.actor.load_weights(a_fn)\n",
    "        print(self.actor.summary())\n",
    "\n",
    "    def load_critic(self, c_fn):\n",
    "        self.critic_1.load_weights(c_fn)\n",
    "        self.critic_target_1.load_weights(c_fn)\n",
    "        self.critic_2.load_weights(c_fn)\n",
    "        self.critic_target_2.load_weights(c_fn)\n",
    "        print(self.critic_1.summary())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.reshape(-1)  # Flatten state\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        next_state = next_state.reshape(-1)  # Flatten next-state\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, self.batch_size)\n",
    "        s = np.array(samples).T\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            np.vstack(s[i, :]).astype(np.float) for i in range(5)\n",
    "        ]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # next state action log probs\n",
    "            means, log_stds = self.actor(next_states)\n",
    "            log_stds = tf.clip_by_value(log_stds, self.log_std_min, self.log_std_max)\n",
    "            next_actions, log_probs = self.process_actions(means, log_stds)\n",
    "\n",
    "            # critics loss\n",
    "            current_q_1 = self.critic_1([states, actions])\n",
    "            current_q_2 = self.critic_2([states, actions])\n",
    "            next_q_1 = self.critic_target_1([next_states, next_actions])\n",
    "            next_q_2 = self.critic_target_2([next_states, next_actions])\n",
    "            next_q_min = tf.math.minimum(next_q_1, next_q_2)\n",
    "            state_values = next_q_min - self.alpha * log_probs\n",
    "            target_qs = tf.stop_gradient(\n",
    "                rewards + state_values * self.gamma * (1.0 - dones)\n",
    "            )\n",
    "            critic_loss_1 = tf.reduce_mean(\n",
    "                0.5 * tf.math.square(current_q_1 - target_qs)\n",
    "            )\n",
    "            critic_loss_2 = tf.reduce_mean(\n",
    "                0.5 * tf.math.square(current_q_2 - target_qs)\n",
    "            )\n",
    "\n",
    "            # current state action log probs\n",
    "            means, log_stds = self.actor(states)\n",
    "            log_stds = tf.clip_by_value(log_stds, self.log_std_min, self.log_std_max)\n",
    "            actions, log_probs = self.process_actions(means, log_stds)\n",
    "\n",
    "            # actor loss\n",
    "            current_q_1 = self.critic_1([states, actions])\n",
    "            current_q_2 = self.critic_2([states, actions])\n",
    "            current_q_min = tf.math.minimum(current_q_1, current_q_2)\n",
    "            actor_loss = tf.reduce_mean(self.alpha * log_probs - current_q_min)\n",
    "\n",
    "            # temperature loss\n",
    "            if self.auto_alpha:\n",
    "                alpha_loss = -tf.reduce_mean(\n",
    "                    (self.log_alpha * tf.stop_gradient(log_probs + self.target_entropy))\n",
    "                )\n",
    "\n",
    "        critic_grad = tape.gradient(\n",
    "            critic_loss_1, self.critic_1.trainable_variables\n",
    "        )  # compute actor gradient\n",
    "        self.critic_optimizer_1.apply_gradients(\n",
    "            zip(critic_grad, self.critic_1.trainable_variables)\n",
    "        )\n",
    "\n",
    "        critic_grad = tape.gradient(\n",
    "            critic_loss_2, self.critic_2.trainable_variables\n",
    "        )  # compute actor gradient\n",
    "        self.critic_optimizer_2.apply_gradients(\n",
    "            zip(critic_grad, self.critic_2.trainable_variables)\n",
    "        )\n",
    "\n",
    "        actor_grad = tape.gradient(\n",
    "            actor_loss, self.actor.trainable_variables\n",
    "        )  # compute actor gradient\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # tensorboard info\n",
    "        self.summaries[\"q1_loss\"] = critic_loss_1\n",
    "        self.summaries[\"q2_loss\"] = critic_loss_2\n",
    "        self.summaries[\"actor_loss\"] = actor_loss\n",
    "\n",
    "        if self.auto_alpha:\n",
    "            # optimize temperature\n",
    "            alpha_grad = tape.gradient(alpha_loss, [self.log_alpha])\n",
    "            self.alpha_optimizer.apply_gradients(zip(alpha_grad, [self.log_alpha]))\n",
    "            self.alpha.assign(tf.exp(self.log_alpha))\n",
    "            # tensorboard info\n",
    "            self.summaries[\"alpha_loss\"] = alpha_loss\n",
    "\n",
    "    def train(self, max_epochs=8000, random_epochs=1000, max_steps=1000, save_freq=50):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        train_log_dir = os.path.join(\"logs\", \"TFRL-Cookbook-Ch4-SAC\", current_time)\n",
    "        summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "        done, use_random, episode, steps, epoch, episode_reward = (\n",
    "            False,\n",
    "            True,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        cur_state = self.env.reset()\n",
    "\n",
    "        while epoch < max_epochs:\n",
    "            if steps > max_steps:\n",
    "                done = True\n",
    "\n",
    "            if done:\n",
    "                episode += 1\n",
    "                print(\n",
    "                    \"episode {}: {} total reward, {} alpha, {} steps, {} epochs\".format(\n",
    "                        episode, episode_reward, self.alpha.numpy(), steps, epoch\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\n",
    "                        \"Main/episode_reward\", episode_reward, step=episode\n",
    "                    )\n",
    "                    tf.summary.scalar(\"Main/episode_steps\", steps, step=episode)\n",
    "\n",
    "                summary_writer.flush()\n",
    "\n",
    "                done, cur_state, steps, episode_reward = False, self.env.reset(), 0, 0\n",
    "                if episode % save_freq == 0:\n",
    "                    self.save_model(\n",
    "                        \"sac_actor_episode{}.h5\".format(episode),\n",
    "                        \"sac_critic_episode{}.h5\".format(episode),\n",
    "                    )\n",
    "\n",
    "            if epoch > random_epochs and len(self.memory) > self.batch_size:\n",
    "                use_random = False\n",
    "\n",
    "            action = self.act(cur_state, use_random=use_random)  # determine action\n",
    "            next_state, reward, done, _ = self.env.step(action[0])  # act on env\n",
    "            # self.env.render(mode='rgb_array')\n",
    "\n",
    "            self.remember(cur_state, action, reward, next_state, done)  # add to memory\n",
    "            self.replay()  # train models through memory replay\n",
    "\n",
    "            update_target_weights(\n",
    "                self.critic_1, self.critic_target_1, tau=self.tau\n",
    "            )  # iterates target model\n",
    "            update_target_weights(self.critic_2, self.critic_target_2, tau=self.tau)\n",
    "\n",
    "            cur_state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            epoch += 1\n",
    "\n",
    "            # Tensorboard update\n",
    "            with summary_writer.as_default():\n",
    "                if len(self.memory) > self.batch_size:\n",
    "                    tf.summary.scalar(\n",
    "                        \"Loss/actor_loss\", self.summaries[\"actor_loss\"], step=epoch\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        \"Loss/q1_loss\", self.summaries[\"q1_loss\"], step=epoch\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        \"Loss/q2_loss\", self.summaries[\"q2_loss\"], step=epoch\n",
    "                    )\n",
    "                    if self.auto_alpha:\n",
    "                        tf.summary.scalar(\n",
    "                            \"Loss/alpha_loss\", self.summaries[\"alpha_loss\"], step=epoch\n",
    "                        )\n",
    "\n",
    "                tf.summary.scalar(\"Stats/alpha\", self.alpha, step=epoch)\n",
    "                if self.auto_alpha:\n",
    "                    tf.summary.scalar(\"Stats/log_alpha\", self.log_alpha, step=epoch)\n",
    "                tf.summary.scalar(\"Stats/q_min\", self.summaries[\"q_min\"], step=epoch)\n",
    "                tf.summary.scalar(\"Stats/q_mean\", self.summaries[\"q_mean\"], step=epoch)\n",
    "                tf.summary.scalar(\"Main/step_reward\", reward, step=epoch)\n",
    "\n",
    "            summary_writer.flush()\n",
    "\n",
    "        self.save_model(\n",
    "            \"sac_actor_final_episode{}.h5\".format(episode),\n",
    "            \"sac_critic_final_episode{}.h5\".format(episode),\n",
    "        )\n",
    "\n",
    "    def test(self, render=True, fps=30, filename=\"test_render.mp4\"):\n",
    "        cur_state, done, rewards = self.env.reset(), False, 0\n",
    "        video = imageio.get_writer(filename, fps=fps)\n",
    "        while not done:\n",
    "            action = self.act(cur_state, test=True)\n",
    "            next_state, reward, done, _ = self.env.step(action[0])\n",
    "            cur_state = next_state\n",
    "            rewards += reward\n",
    "            if render:\n",
    "                video.append_data(self.env.render(mode=\"rgb_array\"))\n",
    "        video.close()\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runner/miniconda/envs/tfrl-cookbook/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Actor\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 186)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "L0 (Dense)                      (None, 64)           11968       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "L1 (Dense)                      (None, 64)           4160        L0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Out_mean (Dense)                (None, 1)            65          L1[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Out_std (Dense)                 (None, 1)            65          L1[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 16,258\n",
      "Trainable params: 16,258\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"Critic\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 186)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 187)          0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Hidden0 (Dense)                 (None, 64)           12032       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Hidden1 (Dense)                 (None, 64)           4160        Hidden0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Out_QVal (Dense)                (None, 1)            65          Hidden1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,257\n",
      "Trainable params: 16,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gym_env = CryptoTradingContinuousEnv()\n",
    "    sac = SAC(gym_env)\n",
    "\n",
    "    # sac.load_actor(\"sac_actor_episodexyz.h5\")\n",
    "    # sac.load_critic(\"sac_critic_episodexyz.h5\")\n",
    "\n",
    "    sac.train(max_epochs=3, random_epochs=1, save_freq=2)\n",
    "    # Comment above line, uncomment the below line for reasonable training\n",
    "    # sac.train(max_epochs=100000, random_epochs=10000, save_freq=50)\n",
    "\n",
    "    # reward = sac.test()\n",
    "    # print(reward)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "tfrl-cookbook",
   "language": "python",
   "name": "tfrl-cookbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
